## Lab 2: running experiments ##
Enhance the model creation process by tracking your experiments and monitoring run metrics. In this lab, learn how to add logging code to your training script, submit an experiment run, monitor that run, and inspect the results in Azure Machine Learning. We are going to perform the following steps in the lab:
* Understand the "normal" train script
* Run the train script local
* Refactor the train script to log metrics to Azure Machine Learning
* Submit an experiment 
* Monitor the run
* Inpect the results in Azure Machine Learning

# Pre-requirements #
1. Familiarize yourself with the concept of Azure Machine Learning by going though [POWERPOINT]
2. Familiarize yourself with the concept of experiments by going though [POWERPOINT]
3. Finished the setup file [01_setup](https://github.com/miquelladeboer/aml-mlops-workshop/blob/master/labs/01_setup.md)


# Understand the non-azure / open source ml model code #
We first start with understanding the training script. The training script is an open source ML model code from https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html.  The dataset used in this example is the 20 newsgroups dataset. It will be automatically downloaded, then cached. The newsgroup datasets contains text documents that are classified into 20 categories.

Open the train.py document to inspect the code.
    
The first step in the code is to load the dataset from the 20 newsgroup dataset. In this example we are only going to use a subset of the categories. Please state the catogories we are going to use:

...

The second step is to extract the features from the text. We do this with a sparse vecorizer. We also clean the data a bit. What is the operation that we do on the data to clean the text?

...

After we have reshaped our data and made sure the feature names are in the right place, we are going to define the algorithm to fit the model. This step is defining the benchmark. We fit the data and make predictions on the test set. To validate out model we need a metric to score the model. There are many metrics we can use. Define in the code the metric that you want to use to validate your model and make sure the print statement will output your metric. (Note: you can define multiple scores if you want. If so, make sure to return these scores.)

...


The last step is to define tha algoritms that we want to fit over our data. In this example we are using 1 classification algoritm to fit the data. We keep track of the metrics of all algoritms, so we can compare the performance and pick the model. Look at the code and whrite down the algoritm that we are going to test.

...

# Run the training locally #
Just to check, we are now going to train the script locally without using Azure ML. 
1. Execute the script `code/explore/train.py`

#  Run the code via Azure ML #
Running the code via Azure ML, we need to excecute two steps. First, we need to refactor the training script. Secondly, we need to create a submit_train file to excecute the train file.

## Refactor the code to capture run metrics in code/explore/train.py
We can caputure the results from our run and log the result to Azure Machine Learning. This way we can keep track of the performance of our models while we are experimenting with different models, parameters, data transformations or feature selections. We can specify for ourselfves what is important to track and log number, graphs and tables to Azure ML, including confusion matrices from SKlearn. For a full overview check the [avaiable metrics to track](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-track-experiments#available-metrics-to-track)


1. Get the run context
    First step is o get the run context. A run represents a single trial of an experiment. Runs are used to monitor the asynchronous execution of a trial, log metrics and store output of the trial, and to analyze results and access artifacts generated by the trial. The get_context() statement returns current service context. We use this method to retrieve the current service context for logging metrics and uploading files.
    ```
    from azureml.core import Run
    run = Run.get_context()
    ```

2. Log the metric in the run
     Logging a metric to a run causes that metric to be stored in the run record in the experiment. You can log the same metric multiple times within a run, the result being considered a vector of that metric. In this lab, we are going to log the accuracy of the model. Later, in future labs we include more advanced logging like graphs and confusion matrices.

    `run.log("accuracy", float(score))`

3. upload the .pkl file to the output folder
    The pkl file contains the model we have trained. We want to upload the model to the Azure Machine Learning Service. We need to create an outputs folder if one is not present yet where we can save the model at the top of the file.
    ```
    import os

    # Define ouputs folder
    OUTPUTSFOLDER = "outputs"

    # create outputs folder if not exists
    if not os.path.exists(OUTPUTSFOLDER):
        os.makedirs(OUTPUTSFOLDER)
    ```
    Next, at the end of the file, once we have trainded our model, we want to save that model in the outputs folder we have just created, by the following code:
    ```
    from sklearn.externals import joblib

    # save .pkl file
    model_name = "model" + ".pkl"
    filename = os.path.join(OUTPUTSFOLDER, model_name)
    joblib.dump(value=clf, filename=filename)
    run.upload_file(name=model_name, path_or_stream=filename)
    ```

4. close the run

    `run.complete()`

5. Execute the refactored script `code/explore/train.py`
As an output you should get the following:
```
Attempted to log scalar metric accuracy:
0.7834441980783444
Attempted to track file modelRandom forest.pkl at outputs/modelRandom forest.pkl
Accuracy  0.783
```

## ALter the train_submit.py file

1. Load required Azureml libraries
    ```
    from azureml.core import Workspace, Experiment
    from azureml.train.estimator import Estimator
    ```

2. Load Azure ML workspace form config file
    ```
    # load Azure ML workspace
    workspace = Workspace.from_config(auth=AzureCliAuthentication())
    ```

3. Create an extimator to define the run configuration
    ```
    # Define Run Configuration
    est = Estimator(
    entry_script='train.py',
    source_directory=os.path.dirname(os.path.realpath(__file__)),
    compute_target='local',
    conda_packages=[
        'pip==20.0.2'
    ],
    pip_packages=[
        'numpy==1.15.4',
        'pandas==0.23.4',
        'scikit-learn==0.20.1',
        'scipy==1.0.0',
        'matplotlib==3.0.2',
        'utils==0.9.0'
    ],
    use_docker=False
    )
    ```

4. Define the ML experiment
    ```
    # Define the ML experiment
    experiment = Experiment(workspace, "newsgroups_train")
    ```

5. Submit the experiment
    ```
    # Submit experiment run, if compute is idle, this may take some time')
    run = experiment.submit(est)

    # wait for run completion of the run, while showing the logs
    run.wait_for_completion(show_output=True)
    ```


6. Go to the portal to inspect the run history

Note: the correct code is already available in codeazureml. In here, all ready to use code is available for the entire workshop.

